{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2722fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in Libraries necessary for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "95e3e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, regexp_tokenize\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from spacy.lang.en import English\n",
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ed9a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de5ef984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pupnsuds90\\AppData\\Local\\Temp\\ipykernel_6980\\3995532456.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Titles = pd.read_csv('Total_Title_Data.csv')\n"
     ]
    }
   ],
   "source": [
    "Titles = pd.read_csv('Total_Title_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee967cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 426870 entries, 0 to 426869\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   EAN                   426870 non-null  int64  \n",
      " 1   Title                 426870 non-null  object \n",
      " 2   Alt Vend              426870 non-null  object \n",
      " 3   Ingram Category Code  425121 non-null  object \n",
      " 4   MJR BISAC             426870 non-null  object \n",
      " 5   BISAC                 426870 non-null  object \n",
      " 6   List                  426870 non-null  float64\n",
      " 7   Annotation            426870 non-null  object \n",
      " 8   Category              426870 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(6)\n",
      "memory usage: 29.3+ MB\n"
     ]
    }
   ],
   "source": [
    "Titles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "79a7a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "Titles['EAN'] = Titles['EAN'].astype('str')\n",
    "Titles['Title'] = Titles['Title'].astype('str')\n",
    "Titles['Alt Vend'] = Titles['Alt Vend'].astype('str')\n",
    "Titles['Ingram Category Code'] = Titles['Ingram Category Code'].astype('str')\n",
    "Titles['MJR BISAC'] = Titles['MJR BISAC'].astype('str')\n",
    "Titles['Annotation'] = Titles['Annotation'].astype('str')\n",
    "Titles['Category'] = Titles['Category'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "26ba1c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 426870 entries, 0 to 426869\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   EAN                   426870 non-null  object \n",
      " 1   Title                 426870 non-null  object \n",
      " 2   Alt Vend              426870 non-null  object \n",
      " 3   Ingram Category Code  426870 non-null  object \n",
      " 4   MJR BISAC             426870 non-null  object \n",
      " 5   BISAC                 426870 non-null  object \n",
      " 6   List                  426870 non-null  float64\n",
      " 7   Annotation            426870 non-null  object \n",
      " 8   Category              426870 non-null  object \n",
      "dtypes: float64(1), object(8)\n",
      "memory usage: 29.3+ MB\n"
     ]
    }
   ],
   "source": [
    "Titles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9fe58991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAN</th>\n",
       "      <th>Title</th>\n",
       "      <th>Alt Vend</th>\n",
       "      <th>Ingram Category Code</th>\n",
       "      <th>MJR BISAC</th>\n",
       "      <th>BISAC</th>\n",
       "      <th>List</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9781803143057</td>\n",
       "      <td>Murder at the Country Club: An absolutely unpu...</td>\n",
       "      <td>6072814</td>\n",
       "      <td>FR</td>\n",
       "      <td>FIC</td>\n",
       "      <td>FIC027110</td>\n",
       "      <td>9.99</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;Kitty Underhay is playing doubles.....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9798440995017</td>\n",
       "      <td>From Sandpaper To Silk (Book One)             ...</td>\n",
       "      <td>6107230</td>\n",
       "      <td>OP</td>\n",
       "      <td>OCC</td>\n",
       "      <td>OCC000000</td>\n",
       "      <td>15.00</td>\n",
       "      <td>This is a very suspenseful, engaging, and rive...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9798885362184</td>\n",
       "      <td>Jason Lang &amp; the iTunes Scandals: The Real-Lif...</td>\n",
       "      <td>6110949</td>\n",
       "      <td>BA</td>\n",
       "      <td>BIO</td>\n",
       "      <td>BIO000000</td>\n",
       "      <td>9.99</td>\n",
       "      <td>&lt;p&gt;Real Life Story About the Gilgo Beach Murde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9781636160887</td>\n",
       "      <td>Izzie's Incredible Imagination                ...</td>\n",
       "      <td>9269037</td>\n",
       "      <td>CL</td>\n",
       "      <td>JUV</td>\n",
       "      <td>JUV001000</td>\n",
       "      <td>9.99</td>\n",
       "      <td>&lt;p&gt;Whether you are floating in space or riding...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9781803810324</td>\n",
       "      <td>Zeppelins in the Searchlights, Sisters of Merc...</td>\n",
       "      <td>6027623</td>\n",
       "      <td>BA</td>\n",
       "      <td>BIO</td>\n",
       "      <td>BIO026000</td>\n",
       "      <td>9.50</td>\n",
       "      <td>&lt;p&gt;Born in 1909 in Hull, Bernard recalls how h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             EAN                                              Title Alt Vend  \\\n",
       "0  9781803143057  Murder at the Country Club: An absolutely unpu...  6072814   \n",
       "1  9798440995017  From Sandpaper To Silk (Book One)             ...  6107230   \n",
       "2  9798885362184  Jason Lang & the iTunes Scandals: The Real-Lif...  6110949   \n",
       "3  9781636160887  Izzie's Incredible Imagination                ...  9269037   \n",
       "4  9781803810324  Zeppelins in the Searchlights, Sisters of Merc...  6027623   \n",
       "\n",
       "  Ingram Category Code MJR BISAC      BISAC   List  \\\n",
       "0                   FR       FIC  FIC027110   9.99   \n",
       "1                   OP       OCC  OCC000000  15.00   \n",
       "2                   BA       BIO  BIO000000   9.99   \n",
       "3                   CL       JUV  JUV001000   9.99   \n",
       "4                   BA       BIO  BIO026000   9.50   \n",
       "\n",
       "                                          Annotation Category  \n",
       "0  <p><strong>Kitty Underhay is playing doubles.....        1  \n",
       "1  This is a very suspenseful, engaging, and rive...        1  \n",
       "2  <p>Real Life Story About the Gilgo Beach Murde...        1  \n",
       "3  <p>Whether you are floating in space or riding...        1  \n",
       "4  <p>Born in 1909 in Hull, Bernard recalls how h...        1  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "466f7782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pupnsuds90\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "Titles['Annotation Text'] = Titles['Annotation'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b7992111",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAN</th>\n",
       "      <th>Title</th>\n",
       "      <th>Alt Vend</th>\n",
       "      <th>Ingram Category Code</th>\n",
       "      <th>MJR BISAC</th>\n",
       "      <th>BISAC</th>\n",
       "      <th>List</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>Category</th>\n",
       "      <th>Annotation Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9781803143057</td>\n",
       "      <td>Murder at the Country Club: An absolutely unpu...</td>\n",
       "      <td>6072814</td>\n",
       "      <td>FR</td>\n",
       "      <td>FIC</td>\n",
       "      <td>FIC027110</td>\n",
       "      <td>9.99</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;Kitty Underhay is playing doubles.....</td>\n",
       "      <td>1</td>\n",
       "      <td>Kitty Underhay is playing doubles... with deat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9798440995017</td>\n",
       "      <td>From Sandpaper To Silk (Book One)             ...</td>\n",
       "      <td>6107230</td>\n",
       "      <td>OP</td>\n",
       "      <td>OCC</td>\n",
       "      <td>OCC000000</td>\n",
       "      <td>15.00</td>\n",
       "      <td>This is a very suspenseful, engaging, and rive...</td>\n",
       "      <td>1</td>\n",
       "      <td>This is a very suspenseful, engaging, and rive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9798885362184</td>\n",
       "      <td>Jason Lang &amp; the iTunes Scandals: The Real-Lif...</td>\n",
       "      <td>6110949</td>\n",
       "      <td>BA</td>\n",
       "      <td>BIO</td>\n",
       "      <td>BIO000000</td>\n",
       "      <td>9.99</td>\n",
       "      <td>&lt;p&gt;Real Life Story About the Gilgo Beach Murde...</td>\n",
       "      <td>1</td>\n",
       "      <td>Real Life Story About the Gilgo Beach Murders ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9781636160887</td>\n",
       "      <td>Izzie's Incredible Imagination                ...</td>\n",
       "      <td>9269037</td>\n",
       "      <td>CL</td>\n",
       "      <td>JUV</td>\n",
       "      <td>JUV001000</td>\n",
       "      <td>9.99</td>\n",
       "      <td>&lt;p&gt;Whether you are floating in space or riding...</td>\n",
       "      <td>1</td>\n",
       "      <td>Whether you are floating in space or riding a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9781803810324</td>\n",
       "      <td>Zeppelins in the Searchlights, Sisters of Merc...</td>\n",
       "      <td>6027623</td>\n",
       "      <td>BA</td>\n",
       "      <td>BIO</td>\n",
       "      <td>BIO026000</td>\n",
       "      <td>9.50</td>\n",
       "      <td>&lt;p&gt;Born in 1909 in Hull, Bernard recalls how h...</td>\n",
       "      <td>1</td>\n",
       "      <td>Born in 1909 in Hull, Bernard recalls how he s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             EAN                                              Title Alt Vend  \\\n",
       "0  9781803143057  Murder at the Country Club: An absolutely unpu...  6072814   \n",
       "1  9798440995017  From Sandpaper To Silk (Book One)             ...  6107230   \n",
       "2  9798885362184  Jason Lang & the iTunes Scandals: The Real-Lif...  6110949   \n",
       "3  9781636160887  Izzie's Incredible Imagination                ...  9269037   \n",
       "4  9781803810324  Zeppelins in the Searchlights, Sisters of Merc...  6027623   \n",
       "\n",
       "  Ingram Category Code MJR BISAC      BISAC   List  \\\n",
       "0                   FR       FIC  FIC027110   9.99   \n",
       "1                   OP       OCC  OCC000000  15.00   \n",
       "2                   BA       BIO  BIO000000   9.99   \n",
       "3                   CL       JUV  JUV001000   9.99   \n",
       "4                   BA       BIO  BIO026000   9.50   \n",
       "\n",
       "                                          Annotation Category  \\\n",
       "0  <p><strong>Kitty Underhay is playing doubles.....        1   \n",
       "1  This is a very suspenseful, engaging, and rive...        1   \n",
       "2  <p>Real Life Story About the Gilgo Beach Murde...        1   \n",
       "3  <p>Whether you are floating in space or riding...        1   \n",
       "4  <p>Born in 1909 in Hull, Bernard recalls how h...        1   \n",
       "\n",
       "                                     Annotation Text  \n",
       "0  Kitty Underhay is playing doubles... with deat...  \n",
       "1  This is a very suspenseful, engaging, and rive...  \n",
       "2  Real Life Story About the Gilgo Beach Murders ...  \n",
       "3  Whether you are floating in space or riding a ...  \n",
       "4  Born in 1909 in Hull, Bernard recalls how he s...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "288c82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Titles['Annotation Text'] = Titles['Annotation Text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "998a22ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset:\n",
      "                  EAN                                              Title  \\\n",
      "144     9781685364816  The Song of &#23433;&#29738; An-Chee          ...   \n",
      "154     9789948817444  &#1571;&#1579;&#1610;&#1585;&#1610;           ...   \n",
      "162     9781647841553  &#26368;&#20986;&#21517;&#30340;&#19968;&#3000...   \n",
      "177     9789948817260  &#1587;&#1576;&#1578;&#1575;&#1580;&#1585;&#15...   \n",
      "188     9789948817482  &#1575;&#1604;&#1576;&#1585;&#1607;&#1575;&#16...   \n",
      "...               ...                                                ...   \n",
      "421445  9791188195107  Best Korean Short Stories Collection &#45824;&...   \n",
      "421447  9791188195282  Best Korean Short Stories Collection 2 &#45824...   \n",
      "422204  9781958313084  Muslimische Frauen und der Hijab-Schleier: Unt...   \n",
      "422312  9781956555257  Trace and Color: Learning Collection Ages 3-6 ...   \n",
      "422628  9781772280333  Navzdy Se Mnou: O Mém U&#268;iteli Rabasovi   ...   \n",
      "\n",
      "       Alt Vend Ingram Category Code MJR BISAC      BISAC   List  \\\n",
      "144     9223758                   MU       MUS  MUS000000   8.99   \n",
      "154     6112990                   HE       SEL  SEL027000   7.95   \n",
      "162     9170656                   HU       HUM  HUM003000  19.99   \n",
      "177     6112990                   FM       FIC  FIC022060  28.95   \n",
      "188     6112990                   CL       JNF  JNF049000  14.95   \n",
      "...         ...                  ...       ...        ...    ...   \n",
      "421445  9037057                   LF       FOR  FOR015000  12.95   \n",
      "421447  9037057                   LF       FOR  FOR015000  12.95   \n",
      "422204  9184014                   RS       REL  REL037000   9.90   \n",
      "422312  9242563                   CL       JNF  JNF006020   9.29   \n",
      "422628  9163945                   RJ       REL  REL040060  15.00   \n",
      "\n",
      "                                               Annotation Category  \\\n",
      "144     <p>Anchee Escapes from the Chinese dynasty pas...        1   \n",
      "154     &#1604;&#1604;&#1602;&#1605;&#1585; &#1585;&#1...        1   \n",
      "162     <p>&#20940;&#40718;&#24180;&#65292;&#20013;&#2...        1   \n",
      "177     &#1605;&#1575; &#1586;&#1575;&#1604;&#1578; (&...        1   \n",
      "188     &#1585;&#1581;&#1604;&#1577; &#1605;&#1606;&#1...        1   \n",
      "...                                                   ...      ...   \n",
      "421445  <p>Meet the very finest of Korean short storie...        3   \n",
      "421447  <p>Meet the very finest of Korean short storie...        3   \n",
      "422204  <p>In den Medien und in der Gesellschaft wird ...        3   \n",
      "422312  <p><strong>Perfect tool for home schooling and...        3   \n",
      "422628  <p>&#65279;&#65279;Michael Laitman strávil ved...        3   \n",
      "\n",
      "                                          Annotation Text  \n",
      "144     Anchee Escapes from the Chinese dynasty past t...  \n",
      "154     للقمر رمزيةٌ خاصة لفهم أنَّ أرواحَنا لا تفنى ح...  \n",
      "162     凌鼎年，中國作家協會會員、世界華文微型小說研究會會長、亞洲微電影學院客座教授，美國紐約商務出...  \n",
      "177     ما زالت (ليليث) تجوس بين الظِّلال، ما زال (لوس...  \n",
      "188     رحلة منطقية مِن البداية، مِن أصول المنطق والتف...  \n",
      "...                                                   ...  \n",
      "421445  Meet the very finest of Korean short stories. ...  \n",
      "421447  Meet the very finest of Korean short stories. ...  \n",
      "422204  In den Medien und in der Gesellschaft wird die...  \n",
      "422312  Perfect tool for home schooling and having fun...  \n",
      "422628  ﻿﻿Michael Laitman strávil vedle svého Učitele ...  \n",
      "\n",
      "[4982 rows x 10 columns]\n",
      "Updated Original Dataset:\n",
      "                  EAN                                              Title  \\\n",
      "0       9781803143057  Murder at the Country Club: An absolutely unpu...   \n",
      "1       9798440995017  From Sandpaper To Silk (Book One)             ...   \n",
      "2       9798885362184  Jason Lang & the iTunes Scandals: The Real-Lif...   \n",
      "3       9781636160887  Izzie's Incredible Imagination                ...   \n",
      "4       9781803810324  Zeppelins in the Searchlights, Sisters of Merc...   \n",
      "...               ...                                                ...   \n",
      "421883  9780228889755  Zoe and the I Can't Monkey                    ...   \n",
      "421884  9780228889762  Zoe and the I Can't Monkey                    ...   \n",
      "421885  9798215456781  Zombie Apocalypse Call Center                 ...   \n",
      "421886  9781837873371  Zone Diet: The Most Effective Weight Loss Tool...   \n",
      "421887  9783036568249  Zoomorphic Arts of Ancient Central Eurasia    ...   \n",
      "\n",
      "       Alt Vend Ingram Category Code MJR BISAC      BISAC   List  \\\n",
      "0       6072814                   FR       FIC  FIC027110   9.99   \n",
      "1       6107230                   OP       OCC  OCC000000  15.00   \n",
      "2       6110949                   BA       BIO  BIO000000   9.99   \n",
      "3       9269037                   CL       JUV  JUV001000   9.99   \n",
      "4       6027623                   BA       BIO  BIO026000   9.50   \n",
      "...         ...                  ...       ...        ...    ...   \n",
      "421883  9076110                   CL       JUV  JUV039140   9.99   \n",
      "421884  9076110                   CL       JUV  JUV039140  17.99   \n",
      "421885  6114132                   FC       FIC  FIC002000  18.95   \n",
      "421886  9250944                   CW       CKB  CKB000000  11.99   \n",
      "421887  9027302                   SO       SOC  SOC000000  70.31   \n",
      "\n",
      "                                               Annotation Category  \\\n",
      "0       <p><strong>Kitty Underhay is playing doubles.....        1   \n",
      "1       This is a very suspenseful, engaging, and rive...        1   \n",
      "2       <p>Real Life Story About the Gilgo Beach Murde...        1   \n",
      "3       <p>Whether you are floating in space or riding...        1   \n",
      "4       <p>Born in 1909 in Hull, Bernard recalls how h...        1   \n",
      "...                                                   ...      ...   \n",
      "421883  <p>When Zoe decides to take the training wheel...        3   \n",
      "421884  <p>When Zoe decides to take the training wheel...        3   \n",
      "421885  <p>Who are you going to call, to survive the z...        3   \n",
      "421886  <p>This book analyzes the blue zone diet, incl...        3   \n",
      "421887  <p>The volume focuses on the zoomorphic art an...        3   \n",
      "\n",
      "                                          Annotation Text  \n",
      "0       Kitty Underhay is playing doubles... with deat...  \n",
      "1       This is a very suspenseful, engaging, and rive...  \n",
      "2       Real Life Story About the Gilgo Beach Murders ...  \n",
      "3       Whether you are floating in space or riding a ...  \n",
      "4       Born in 1909 in Hull, Bernard recalls how he s...  \n",
      "...                                                   ...  \n",
      "421883  When Zoe decides to take the training wheels o...  \n",
      "421884  When Zoe decides to take the training wheels o...  \n",
      "421885  Who are you going to call, to survive the zomb...  \n",
      "421886  This book analyzes the blue zone diet, includi...  \n",
      "421887  The volume focuses on the zoomorphic art and d...  \n",
      "\n",
      "[421888 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the regex pattern\n",
    "pattern = r'&#\\d+;'\n",
    "\n",
    "# Create a new dataset for titles matching the pattern\n",
    "new_dataset = Titles[Titles['Title'].str.contains(pattern, regex=True)].copy()\n",
    "\n",
    "# Remove the matching titles from the original dataset\n",
    "Titles = Titles[~Titles['Title'].str.contains(pattern, regex=True)].reset_index(drop=True)\n",
    "\n",
    "# Print the new dataset containing titles matching the pattern\n",
    "print(\"New Dataset:\")\n",
    "print(new_dataset)\n",
    "\n",
    "# Print the updated original dataset without titles matching the pattern\n",
    "print(\"Updated Original Dataset:\")\n",
    "print(Titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "018164db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAN</th>\n",
       "      <th>Title</th>\n",
       "      <th>Alt Vend</th>\n",
       "      <th>Ingram Category Code</th>\n",
       "      <th>MJR BISAC</th>\n",
       "      <th>BISAC</th>\n",
       "      <th>List</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>Category</th>\n",
       "      <th>Annotation Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>9781685364816</td>\n",
       "      <td>The Song of &amp;#23433;&amp;#29738; An-Chee          ...</td>\n",
       "      <td>9223758</td>\n",
       "      <td>MU</td>\n",
       "      <td>MUS</td>\n",
       "      <td>MUS000000</td>\n",
       "      <td>8.99</td>\n",
       "      <td>&lt;p&gt;Anchee Escapes from the Chinese dynasty pas...</td>\n",
       "      <td>1</td>\n",
       "      <td>Anchee Escapes from the Chinese dynasty past t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>9789948817444</td>\n",
       "      <td>&amp;#1571;&amp;#1579;&amp;#1610;&amp;#1585;&amp;#1610;           ...</td>\n",
       "      <td>6112990</td>\n",
       "      <td>HE</td>\n",
       "      <td>SEL</td>\n",
       "      <td>SEL027000</td>\n",
       "      <td>7.95</td>\n",
       "      <td>&amp;#1604;&amp;#1604;&amp;#1602;&amp;#1605;&amp;#1585; &amp;#1585;&amp;#1...</td>\n",
       "      <td>1</td>\n",
       "      <td>للقمر رمزيةٌ خاصة لفهم أنَّ أرواحَنا لا تفنى ح...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>9781647841553</td>\n",
       "      <td>&amp;#26368;&amp;#20986;&amp;#21517;&amp;#30340;&amp;#19968;&amp;#3000...</td>\n",
       "      <td>9170656</td>\n",
       "      <td>HU</td>\n",
       "      <td>HUM</td>\n",
       "      <td>HUM003000</td>\n",
       "      <td>19.99</td>\n",
       "      <td>&lt;p&gt;&amp;#20940;&amp;#40718;&amp;#24180;&amp;#65292;&amp;#20013;&amp;#2...</td>\n",
       "      <td>1</td>\n",
       "      <td>凌鼎年，中國作家協會會員、世界華文微型小說研究會會長、亞洲微電影學院客座教授，美國紐約商務出...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>9789948817260</td>\n",
       "      <td>&amp;#1587;&amp;#1576;&amp;#1578;&amp;#1575;&amp;#1580;&amp;#1585;&amp;#15...</td>\n",
       "      <td>6112990</td>\n",
       "      <td>FM</td>\n",
       "      <td>FIC</td>\n",
       "      <td>FIC022060</td>\n",
       "      <td>28.95</td>\n",
       "      <td>&amp;#1605;&amp;#1575; &amp;#1586;&amp;#1575;&amp;#1604;&amp;#1578; (&amp;...</td>\n",
       "      <td>1</td>\n",
       "      <td>ما زالت (ليليث) تجوس بين الظِّلال، ما زال (لوس...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>9789948817482</td>\n",
       "      <td>&amp;#1575;&amp;#1604;&amp;#1576;&amp;#1585;&amp;#1607;&amp;#1575;&amp;#16...</td>\n",
       "      <td>6112990</td>\n",
       "      <td>CL</td>\n",
       "      <td>JNF</td>\n",
       "      <td>JNF049000</td>\n",
       "      <td>14.95</td>\n",
       "      <td>&amp;#1585;&amp;#1581;&amp;#1604;&amp;#1577; &amp;#1605;&amp;#1606;&amp;#1...</td>\n",
       "      <td>1</td>\n",
       "      <td>رحلة منطقية مِن البداية، مِن أصول المنطق والتف...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421445</th>\n",
       "      <td>9791188195107</td>\n",
       "      <td>Best Korean Short Stories Collection &amp;#45824;&amp;...</td>\n",
       "      <td>9037057</td>\n",
       "      <td>LF</td>\n",
       "      <td>FOR</td>\n",
       "      <td>FOR015000</td>\n",
       "      <td>12.95</td>\n",
       "      <td>&lt;p&gt;Meet the very finest of Korean short storie...</td>\n",
       "      <td>3</td>\n",
       "      <td>Meet the very finest of Korean short stories. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421447</th>\n",
       "      <td>9791188195282</td>\n",
       "      <td>Best Korean Short Stories Collection 2 &amp;#45824...</td>\n",
       "      <td>9037057</td>\n",
       "      <td>LF</td>\n",
       "      <td>FOR</td>\n",
       "      <td>FOR015000</td>\n",
       "      <td>12.95</td>\n",
       "      <td>&lt;p&gt;Meet the very finest of Korean short storie...</td>\n",
       "      <td>3</td>\n",
       "      <td>Meet the very finest of Korean short stories. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422204</th>\n",
       "      <td>9781958313084</td>\n",
       "      <td>Muslimische Frauen und der Hijab-Schleier: Unt...</td>\n",
       "      <td>9184014</td>\n",
       "      <td>RS</td>\n",
       "      <td>REL</td>\n",
       "      <td>REL037000</td>\n",
       "      <td>9.90</td>\n",
       "      <td>&lt;p&gt;In den Medien und in der Gesellschaft wird ...</td>\n",
       "      <td>3</td>\n",
       "      <td>In den Medien und in der Gesellschaft wird die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422312</th>\n",
       "      <td>9781956555257</td>\n",
       "      <td>Trace and Color: Learning Collection Ages 3-6 ...</td>\n",
       "      <td>9242563</td>\n",
       "      <td>CL</td>\n",
       "      <td>JNF</td>\n",
       "      <td>JNF006020</td>\n",
       "      <td>9.29</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;Perfect tool for home schooling and...</td>\n",
       "      <td>3</td>\n",
       "      <td>Perfect tool for home schooling and having fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422628</th>\n",
       "      <td>9781772280333</td>\n",
       "      <td>Navzdy Se Mnou: O Mém U&amp;#268;iteli Rabasovi   ...</td>\n",
       "      <td>9163945</td>\n",
       "      <td>RJ</td>\n",
       "      <td>REL</td>\n",
       "      <td>REL040060</td>\n",
       "      <td>15.00</td>\n",
       "      <td>&lt;p&gt;&amp;#65279;&amp;#65279;Michael Laitman strávil ved...</td>\n",
       "      <td>3</td>\n",
       "      <td>﻿﻿Michael Laitman strávil vedle svého Učitele ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4982 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  EAN                                              Title  \\\n",
       "144     9781685364816  The Song of &#23433;&#29738; An-Chee          ...   \n",
       "154     9789948817444  &#1571;&#1579;&#1610;&#1585;&#1610;           ...   \n",
       "162     9781647841553  &#26368;&#20986;&#21517;&#30340;&#19968;&#3000...   \n",
       "177     9789948817260  &#1587;&#1576;&#1578;&#1575;&#1580;&#1585;&#15...   \n",
       "188     9789948817482  &#1575;&#1604;&#1576;&#1585;&#1607;&#1575;&#16...   \n",
       "...               ...                                                ...   \n",
       "421445  9791188195107  Best Korean Short Stories Collection &#45824;&...   \n",
       "421447  9791188195282  Best Korean Short Stories Collection 2 &#45824...   \n",
       "422204  9781958313084  Muslimische Frauen und der Hijab-Schleier: Unt...   \n",
       "422312  9781956555257  Trace and Color: Learning Collection Ages 3-6 ...   \n",
       "422628  9781772280333  Navzdy Se Mnou: O Mém U&#268;iteli Rabasovi   ...   \n",
       "\n",
       "       Alt Vend Ingram Category Code MJR BISAC      BISAC   List  \\\n",
       "144     9223758                   MU       MUS  MUS000000   8.99   \n",
       "154     6112990                   HE       SEL  SEL027000   7.95   \n",
       "162     9170656                   HU       HUM  HUM003000  19.99   \n",
       "177     6112990                   FM       FIC  FIC022060  28.95   \n",
       "188     6112990                   CL       JNF  JNF049000  14.95   \n",
       "...         ...                  ...       ...        ...    ...   \n",
       "421445  9037057                   LF       FOR  FOR015000  12.95   \n",
       "421447  9037057                   LF       FOR  FOR015000  12.95   \n",
       "422204  9184014                   RS       REL  REL037000   9.90   \n",
       "422312  9242563                   CL       JNF  JNF006020   9.29   \n",
       "422628  9163945                   RJ       REL  REL040060  15.00   \n",
       "\n",
       "                                               Annotation Category  \\\n",
       "144     <p>Anchee Escapes from the Chinese dynasty pas...        1   \n",
       "154     &#1604;&#1604;&#1602;&#1605;&#1585; &#1585;&#1...        1   \n",
       "162     <p>&#20940;&#40718;&#24180;&#65292;&#20013;&#2...        1   \n",
       "177     &#1605;&#1575; &#1586;&#1575;&#1604;&#1578; (&...        1   \n",
       "188     &#1585;&#1581;&#1604;&#1577; &#1605;&#1606;&#1...        1   \n",
       "...                                                   ...      ...   \n",
       "421445  <p>Meet the very finest of Korean short storie...        3   \n",
       "421447  <p>Meet the very finest of Korean short storie...        3   \n",
       "422204  <p>In den Medien und in der Gesellschaft wird ...        3   \n",
       "422312  <p><strong>Perfect tool for home schooling and...        3   \n",
       "422628  <p>&#65279;&#65279;Michael Laitman strávil ved...        3   \n",
       "\n",
       "                                          Annotation Text  \n",
       "144     Anchee Escapes from the Chinese dynasty past t...  \n",
       "154     للقمر رمزيةٌ خاصة لفهم أنَّ أرواحَنا لا تفنى ح...  \n",
       "162     凌鼎年，中國作家協會會員、世界華文微型小說研究會會長、亞洲微電影學院客座教授，美國紐約商務出...  \n",
       "177     ما زالت (ليليث) تجوس بين الظِّلال، ما زال (لوس...  \n",
       "188     رحلة منطقية مِن البداية، مِن أصول المنطق والتف...  \n",
       "...                                                   ...  \n",
       "421445  Meet the very finest of Korean short stories. ...  \n",
       "421447  Meet the very finest of Korean short stories. ...  \n",
       "422204  In den Medien und in der Gesellschaft wird die...  \n",
       "422312  Perfect tool for home schooling and having fun...  \n",
       "422628  ﻿﻿Michael Laitman strávil vedle svého Učitele ...  \n",
       "\n",
       "[4982 rows x 10 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9684fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove 'middle' category, representing 'uncertain' classifications\n",
    "Titles_Binary = Titles[Titles['Category'].isin(['1','3'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d6017f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Titles_Binary = Titles_Binary[['Title', 'Ingram Category Code', 'MJR BISAC', 'BISAC', 'Annotation Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5b498e0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "pipe = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\", tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "39177634",
   "metadata": {},
   "outputs": [],
   "source": [
    "Titles_Test = Titles_Binary.loc[0:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b310052e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Ingram Category Code</th>\n",
       "      <th>MJR BISAC</th>\n",
       "      <th>BISAC</th>\n",
       "      <th>Annotation Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Murder at the Country Club: An absolutely unpu...</td>\n",
       "      <td>FR</td>\n",
       "      <td>FIC</td>\n",
       "      <td>FIC027110</td>\n",
       "      <td>Kitty Underhay is playing doubles... with deat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From Sandpaper To Silk (Book One)             ...</td>\n",
       "      <td>OP</td>\n",
       "      <td>OCC</td>\n",
       "      <td>OCC000000</td>\n",
       "      <td>This is a very suspenseful, engaging, and rive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jason Lang &amp; the iTunes Scandals: The Real-Lif...</td>\n",
       "      <td>BA</td>\n",
       "      <td>BIO</td>\n",
       "      <td>BIO000000</td>\n",
       "      <td>Real Life Story About the Gilgo Beach Murders ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Izzie's Incredible Imagination                ...</td>\n",
       "      <td>CL</td>\n",
       "      <td>JUV</td>\n",
       "      <td>JUV001000</td>\n",
       "      <td>Whether you are floating in space or riding a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zeppelins in the Searchlights, Sisters of Merc...</td>\n",
       "      <td>BA</td>\n",
       "      <td>BIO</td>\n",
       "      <td>BIO026000</td>\n",
       "      <td>Born in 1909 in Hull, Bernard recalls how he s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421883</th>\n",
       "      <td>Zoe and the I Can't Monkey                    ...</td>\n",
       "      <td>CL</td>\n",
       "      <td>JUV</td>\n",
       "      <td>JUV039140</td>\n",
       "      <td>When Zoe decides to take the training wheels o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421884</th>\n",
       "      <td>Zoe and the I Can't Monkey                    ...</td>\n",
       "      <td>CL</td>\n",
       "      <td>JUV</td>\n",
       "      <td>JUV039140</td>\n",
       "      <td>When Zoe decides to take the training wheels o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421885</th>\n",
       "      <td>Zombie Apocalypse Call Center                 ...</td>\n",
       "      <td>FC</td>\n",
       "      <td>FIC</td>\n",
       "      <td>FIC002000</td>\n",
       "      <td>Who are you going to call, to survive the zomb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421886</th>\n",
       "      <td>Zone Diet: The Most Effective Weight Loss Tool...</td>\n",
       "      <td>CW</td>\n",
       "      <td>CKB</td>\n",
       "      <td>CKB000000</td>\n",
       "      <td>This book analyzes the blue zone diet, includi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421887</th>\n",
       "      <td>Zoomorphic Arts of Ancient Central Eurasia    ...</td>\n",
       "      <td>SO</td>\n",
       "      <td>SOC</td>\n",
       "      <td>SOC000000</td>\n",
       "      <td>The volume focuses on the zoomorphic art and d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>395828 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "0       Murder at the Country Club: An absolutely unpu...   \n",
       "1       From Sandpaper To Silk (Book One)             ...   \n",
       "2       Jason Lang & the iTunes Scandals: The Real-Lif...   \n",
       "3       Izzie's Incredible Imagination                ...   \n",
       "4       Zeppelins in the Searchlights, Sisters of Merc...   \n",
       "...                                                   ...   \n",
       "421883  Zoe and the I Can't Monkey                    ...   \n",
       "421884  Zoe and the I Can't Monkey                    ...   \n",
       "421885  Zombie Apocalypse Call Center                 ...   \n",
       "421886  Zone Diet: The Most Effective Weight Loss Tool...   \n",
       "421887  Zoomorphic Arts of Ancient Central Eurasia    ...   \n",
       "\n",
       "       Ingram Category Code MJR BISAC      BISAC  \\\n",
       "0                        FR       FIC  FIC027110   \n",
       "1                        OP       OCC  OCC000000   \n",
       "2                        BA       BIO  BIO000000   \n",
       "3                        CL       JUV  JUV001000   \n",
       "4                        BA       BIO  BIO026000   \n",
       "...                     ...       ...        ...   \n",
       "421883                   CL       JUV  JUV039140   \n",
       "421884                   CL       JUV  JUV039140   \n",
       "421885                   FC       FIC  FIC002000   \n",
       "421886                   CW       CKB  CKB000000   \n",
       "421887                   SO       SOC  SOC000000   \n",
       "\n",
       "                                          Annotation Text  \n",
       "0       Kitty Underhay is playing doubles... with deat...  \n",
       "1       This is a very suspenseful, engaging, and rive...  \n",
       "2       Real Life Story About the Gilgo Beach Murders ...  \n",
       "3       Whether you are floating in space or riding a ...  \n",
       "4       Born in 1909 in Hull, Bernard recalls how he s...  \n",
       "...                                                   ...  \n",
       "421883  When Zoe decides to take the training wheels o...  \n",
       "421884  When Zoe decides to take the training wheels o...  \n",
       "421885  Who are you going to call, to survive the zomb...  \n",
       "421886  This book analyzes the blue zone diet, includi...  \n",
       "421887  The volume focuses on the zoomorphic art and d...  \n",
       "\n",
       "[395828 rows x 5 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Titles_Binary.dropna(inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a2130f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65cf6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  42%|█████████████████████████▍                                   | 110/264 [21:32:02<30:35:04, 714.96s/it]"
     ]
    }
   ],
   "source": [
    "Titles_Binary['Language'] = \"\"\n",
    "Titles_Binary['Score'] = \"\"\n",
    "\n",
    "# Batch processing parameters\n",
    "batch_size = 1500  # Adjust the batch size based on your system's capacity\n",
    "\n",
    "# Calculate the total number of batches\n",
    "num_batches = (len(Titles_Binary) - 1) // batch_size + 1\n",
    "\n",
    "# Iterate over the dataset in batches\n",
    "for batch_index, start_index in tqdm(enumerate(range(0, len(Titles_Binary), batch_size), 1), total=num_batches, desc='Processing'):\n",
    "    end_index = min(start_index + batch_size, len(Titles_Binary))\n",
    "    batch_texts = Titles_Binary['Annotation Text'][start_index:end_index].tolist()\n",
    "\n",
    "    # Encode the batch of texts\n",
    "    encoded_inputs = tokenizer.batch_encode_plus(\n",
    "        batch_texts,\n",
    "        truncation=True,\n",
    "        max_length=5,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Make predictions for the batch\n",
    "    outputs = pipe.model(**encoded_inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_indices = logits.argmax(dim=1).tolist()\n",
    "\n",
    "    # Apply softmax to obtain confidence scores\n",
    "    scores = F.softmax(logits, dim=1)\n",
    "\n",
    "    # Update the corresponding rows in the dataset\n",
    "    for i, index in enumerate(range(start_index, end_index)):\n",
    "        predicted_class_idx = predicted_class_indices[i]\n",
    "        language = pipe.model.config.id2label[predicted_class_idx]\n",
    "        score = scores[i][predicted_class_idx].item()\n",
    "\n",
    "        Titles_Binary.at[index, 'Language'] = language\n",
    "        Titles_Binary.at[index, 'Score'] = score\n",
    "\n",
    "# Print the updated dataset\n",
    "print(Titles_Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6415796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:   0%|                                                                        | 0/40 [01:27<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 34\u001b[0m\n\u001b[0;32m     25\u001b[0m encoded_inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m     26\u001b[0m     batch_texts,\n\u001b[0;32m     27\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Make predictions for the batch\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m outputs \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_inputs)\n\u001b[0;32m     35\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     36\u001b[0m predicted_class_indices \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:1218\u001b[0m, in \u001b[0;36mXLMRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1218\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1229\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1230\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:848\u001b[0m, in \u001b[0;36mXLMRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    839\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    841\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    842\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    843\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    846\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    847\u001b[0m )\n\u001b[1;32m--> 848\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    861\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:521\u001b[0m, in \u001b[0;36mXLMRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    512\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    513\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    514\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    518\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:406\u001b[0m, in \u001b[0;36mXLMRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    396\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    403\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:333\u001b[0m, in \u001b[0;36mXLMRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    325\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    331\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    332\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 333\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    343\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:220\u001b[0m, in \u001b[0;36mXLMRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[1;32m--> 220\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    222\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;66;03m# Further calls to cross_attention layer can then reuse all cross-attention\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\u001b[39;00m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;66;03m# if encoder bi-directional self-attention `past_key_value` is always `None`\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Titles_Binary['Language'] = \"\"\n",
    "Titles_Binary['Score'] = \"\"\n",
    "\n",
    "# Batch processing parameters\n",
    "batch_size = 100  # Adjust the batch size based on your system's capacity\n",
    "chunk_size = 10000  # Adjust the chunk size based on the available memory\n",
    "\n",
    "# Calculate the total number of chunks\n",
    "num_chunks = (len(Titles_Binary) - 1) // chunk_size + 1\n",
    "\n",
    "# Iterate over the dataset in chunks\n",
    "for chunk_index, start_index in tqdm(enumerate(range(0, len(Titles_Binary), chunk_size), 1), total=num_chunks, desc='Processing Chunks'):\n",
    "    end_index = min(start_index + chunk_size, len(Titles_Binary))\n",
    "    chunk_texts = Titles_Binary['Annotation Text'][start_index:end_index].tolist()\n",
    "\n",
    "    # Calculate the total number of batches within the current chunk\n",
    "    num_batches = (len(chunk_texts) - 1) // batch_size + 1\n",
    "\n",
    "    # Iterate over the chunk in batches\n",
    "    for batch_index, batch_start_index in enumerate(range(0, len(chunk_texts), batch_size), 1):\n",
    "        batch_end_index = min(batch_start_index + batch_size, len(chunk_texts))\n",
    "        batch_texts = chunk_texts[batch_start_index:batch_end_index]\n",
    "\n",
    "        # Encode the batch of texts\n",
    "        encoded_inputs = tokenizer.batch_encode_plus(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            max_length=15,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Make predictions for the batch\n",
    "        outputs = pipe.model(**encoded_inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_indices = logits.argmax(dim=1).tolist()\n",
    "\n",
    "        # Apply softmax to obtain confidence scores\n",
    "        scores = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Update the corresponding rows in the dataset\n",
    "        for i, index in enumerate(range(start_index + batch_start_index, start_index + batch_end_index)):\n",
    "            predicted_class_idx = predicted_class_indices[i]\n",
    "            language = pipe.model.config.id2label[predicted_class_idx]\n",
    "            score = scores[i][predicted_class_idx].item()\n",
    "\n",
    "            Titles_Binary.at[index, 'Language'] = language\n",
    "            Titles_Binary.at[index, 'Score'] = score\n",
    "\n",
    "# Print the updated dataset\n",
    "print(Titles_Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b2727",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Empty Columns for Saving Language Prediction and Score\n",
    "Titles_Test['Language'] = \"\"\n",
    "Titles_Test['Score'] = \"\"\n",
    "\n",
    "for index, row in tqdm(Titles.iterrows(), total=len(Titles_Test), desc='Processing'):\n",
    "    text = row['Annotation Text']\n",
    "\n",
    "    # encode_plus to add additional qualifiers to speed up latency\n",
    "    encoded_input = tokenizer.encode_plus(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=5,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Make predictions using the encoded input\n",
    "    output = pipe.model(**encoded_input)\n",
    "    logits = output.logits\n",
    "    predicted_class_idx = logits.argmax(dim=1).item()\n",
    "\n",
    "    # Apply softmax to obtain confidence scores\n",
    "    scores = F.softmax(logits, dim=1)[0]\n",
    "\n",
    "    # Get the language label and confidence score from the pipeline's label list and scores\n",
    "    language = pipe.model.config.id2label[predicted_class_idx]\n",
    "    score = scores[predicted_class_idx].item()\n",
    "\n",
    "    # Update the corresponding row in the dataset\n",
    "    Titles_Test.at[index, 'language'] = language\n",
    "    Titles_Test.at[index, 'score'] = score\n",
    "\n",
    "\n",
    "Titles_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01205f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c69d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Titles.loc[470,'Annotation Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef15c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Titles['PredictedLanguage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Identify Titles in the datset with the code pattern and remove them from the dataframe\n",
    "pattern = r'&#\\d+;'\n",
    "Titles = Titles[~Titles['Title'].str.contains(pattern, regex=True)]\n",
    "\n",
    "#Reset the index of the dataframe\n",
    "Titles = Titles.reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
